{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b919447b",
   "metadata": {},
   "source": [
    "# Task6_FinTech_Vendor_Scorecard.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c48b2",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839e38c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Objective: Develop a FinTech Vendor Scorecard by combining NER extractions with Telegram metadata.\n",
    "\n",
    "# --- Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- Step 1: Install Necessary Libraries ---\n",
    "!pip install transformers pandas numpy\n",
    "\n",
    "# --- Step 2: Import Libraries ---\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone # Import timezone for robust datetime handling\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re # For price extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf6766",
   "metadata": {},
   "source": [
    "## --- Configuration ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97304a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: Adjust 'colab_projects/EthioMart_NER' to your desired base path in Google Drive\n",
    "DRIVE_PROJECT_BASE_PATH = \"/content/drive/MyDrive/colab_projects/EthioMart_NER\"\n",
    "\n",
    "# Path to your preprocessed data file\n",
    "PREPROCESSED_DATA_PATH = os.path.join(DRIVE_PROJECT_BASE_PATH, \"data/preprocessed_data/preprocessed_amharic_ecommerce_messages.json\")\n",
    "\n",
    "# Path to the best performing NER model saved in your Google Drive from Task 4\n",
    "# This path should ideally be read from a file generated by Task 4, or hardcoded if Task 4 already ran.\n",
    "BEST_MODEL_PATH = os.path.join(DRIVE_PROJECT_BASE_PATH, \"XLM-R-Amharic-NER_ner_output/final_model\")\n",
    "# Alternatively, read from file if it exists:\n",
    "# try:\n",
    "#     with open(os.path.join(DRIVE_PROJECT_BASE_PATH, \"best_model_path.txt\"), \"r\") as f:\n",
    "#         BEST_MODEL_PATH = f.read().strip()\n",
    "#     print(f\"Loaded best model path from file: {BEST_MODEL_PATH}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"best_model_path.txt not found. Using default BEST_MODEL_PATH.\")\n",
    "\n",
    "\n",
    "# Define your entity types (must match what you used for training)\n",
    "LABEL_NAMES = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
    "\n",
    "# --- Step 3: Load Preprocessed Data ---\n",
    "print(f\"Loading preprocessed data from: {PREPROCESSED_DATA_PATH}\")\n",
    "all_messages_data = [] # Initialize to an empty list to prevent NameError if file not found\n",
    "try:\n",
    "    with open(PREPROCESSED_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        all_messages_data = json.load(f)\n",
    "    print(f\"Successfully loaded {len(all_messages_data)} messages.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {PREPROCESSED_DATA_PATH}. Please ensure it's uploaded or path is correct.\")\n",
    "    # Do not exit(), let the script continue with empty data if file not found\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON from {PREPROCESSED_DATA_PATH}: {e}\")\n",
    "    # Do not exit(), let the script continue with empty data if JSON is malformed\n",
    "\n",
    "# --- Step 4: Load the Best Fine-Tuned NER Model ---\n",
    "print(f\"Loading best NER model from: {BEST_MODEL_PATH}\")\n",
    "try:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(BEST_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_PATH)\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "    print(\"NER model loaded successfully.\")\n",
    "    model_loaded_for_scorecard = True\n",
    "except Exception as e:\n",
    "    print(f\"Error loading NER model: {e}. Skipping scorecard calculation.\")\n",
    "    model_loaded_for_scorecard = False\n",
    "\n",
    "\n",
    "if model_loaded_for_scorecard:\n",
    "    # --- Step 5: NER Inference and Data Augmentation ---\n",
    "    print(\"\\nRunning NER inference on all preprocessed messages...\")\n",
    "\n",
    "    # Function to extract numerical price from NER output\n",
    "    def extract_numerical_price(ner_output):\n",
    "        price_entities = []\n",
    "        for entity in ner_output:\n",
    "            if entity['entity_group'] == 'PRICE':\n",
    "                price_text = entity['word'].replace(',', '').lower()\n",
    "                numbers = re.findall(r'\\d+\\.?\\d*', price_text)\n",
    "                if numbers:\n",
    "                    try:\n",
    "                        price_value = float(numbers[0])\n",
    "                        if 'ሺህ' in price_text or 'thousand' in price_text:\n",
    "                            price_value *= 1000\n",
    "                        price_entities.append(price_value)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        if price_entities:\n",
    "            return np.mean(price_entities)\n",
    "        return None\n",
    "\n",
    "    # Dictionary to store data grouped by channel\n",
    "    channel_data = {}\n",
    "\n",
    "    for i, message in enumerate(all_messages_data):\n",
    "        if (i + 1) % 5000 == 0: # Print progress more frequently for large datasets\n",
    "            print(f\"Processed {i+1}/{len(all_messages_data)} messages for scorecard NER.\")\n",
    "\n",
    "        channel_id = message.get('channel_id')\n",
    "        channel_name = message.get('channel_name', f\"Channel_{channel_id}\")\n",
    "        cleaned_text = message.get('cleaned_text', '')\n",
    "        views = message.get('views', 0)\n",
    "        timestamp_str = message.get('date')\n",
    "\n",
    "        if not channel_id or not cleaned_text:\n",
    "            continue\n",
    "\n",
    "        if channel_id not in channel_data:\n",
    "            channel_data[channel_id] = {\n",
    "                'name': channel_name,\n",
    "                'posts': [],\n",
    "                'first_post_date': datetime.max.replace(tzinfo=timezone.utc), # Initialize as timezone-aware UTC\n",
    "                'last_post_date': datetime.min.replace(tzinfo=timezone.utc),  # Initialize as timezone-aware UTC\n",
    "                'total_views': 0,\n",
    "                'total_prices': 0,\n",
    "                'price_count': 0,\n",
    "                'top_post_views': -1,\n",
    "                'top_post_details': {}\n",
    "            }\n",
    "\n",
    "        # Perform NER for scorecard\n",
    "        ner_results = []\n",
    "        try:\n",
    "            ner_results = ner_pipeline(cleaned_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: NER inference failed for message ID {message.get('id')} for scorecard: {e}\")\n",
    "            ner_results = []\n",
    "        \n",
    "        numerical_price = extract_numerical_price(ner_results)\n",
    "\n",
    "        post_timestamp = None\n",
    "        if timestamp_str:\n",
    "            try:\n",
    "                dt_object = datetime.fromisoformat(timestamp_str)\n",
    "                if dt_object.tzinfo is None: # If naive, assume UTC or add a default\n",
    "                    post_timestamp = dt_object.replace(tzinfo=timezone.utc) # Make it aware\n",
    "                else:\n",
    "                    post_timestamp = dt_object.astimezone(timezone.utc) # Convert to UTC if already aware\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not parse timestamp {timestamp_str} for message ID {message.get('id')}. Skipping timestamp for this post.\")\n",
    "                post_timestamp = None\n",
    "\n",
    "        post_details = {\n",
    "            'id': message['id'],\n",
    "            'text': cleaned_text,\n",
    "            'views': views,\n",
    "            'timestamp': post_timestamp,\n",
    "            'ner_entities': ner_results,\n",
    "            'numerical_price': numerical_price\n",
    "        }\n",
    "        channel_data[channel_id]['posts'].append(post_details)\n",
    "\n",
    "        channel_data[channel_id]['total_views'] += views\n",
    "        if numerical_price is not None:\n",
    "            channel_data[channel_id]['total_prices'] += numerical_price\n",
    "            channel_data[channel_id]['price_count'] += 1\n",
    "\n",
    "        if views > channel_data[channel_id]['top_post_views']:\n",
    "            channel_data[channel_id]['top_post_views'] = views\n",
    "            top_product = next((e['word'] for e in ner_results if e['entity_group'] == 'PRODUCT'), 'N/A')\n",
    "            top_price = numerical_price if numerical_price is not None else 'N/A'\n",
    "            channel_data[channel_id]['top_post_details'] = {\n",
    "                'message_id': message['id'],\n",
    "                'views': views,\n",
    "                'product': top_product,\n",
    "                'price': top_price\n",
    "            }\n",
    "\n",
    "        if post_details['timestamp']:\n",
    "            if post_details['timestamp'] < channel_data[channel_id]['first_post_date']:\n",
    "                channel_data[channel_id]['first_post_date'] = post_details['timestamp']\n",
    "            if post_details['timestamp'] > channel_data[channel_id]['last_post_date']:\n",
    "                channel_data[channel_id]['last_post_date'] = post_details['timestamp']\n",
    "\n",
    "    print(\"NER inference for scorecard complete.\")\n",
    "\n",
    "    # --- Step 6: Calculate Key Vendor Metrics and Lending Score ---\n",
    "    vendor_scores = []\n",
    "    print(\"\\nCalculating vendor metrics and lending scores...\")\n",
    "    if not channel_data:\n",
    "        print(\"No channel data available to calculate metrics.\")\n",
    "    else:\n",
    "        for c_id, data in channel_data.items():\n",
    "            num_posts = len(data['posts'])\n",
    "            \n",
    "            activity_duration_days = 0\n",
    "            if data['first_post_date'] != datetime.max.replace(tzinfo=timezone.utc) and data['last_post_date'] != datetime.min.replace(tzinfo=timezone.utc):\n",
    "                activity_duration_days = (data['last_post_date'] - data['first_post_date']).days\n",
    "            \n",
    "            posting_frequency_per_week = 0\n",
    "            if activity_duration_days > 0:\n",
    "                posting_frequency_per_week = (num_posts / activity_duration_days) * 7\n",
    "            elif num_posts > 0:\n",
    "                posting_frequency_per_week = num_posts\n",
    "\n",
    "            average_views_per_post = data['total_views'] / num_posts if num_posts > 0 else 0\n",
    "            average_price_point = data['total_prices'] / data['price_count'] if data['price_count'] > 0 else 0\n",
    "\n",
    "            max_posting_freq = 50.0\n",
    "            max_avg_views = 10000.0\n",
    "            max_avg_price = 50000.0\n",
    "\n",
    "            normalized_posting_freq = min(posting_frequency_per_week / max_posting_freq, 1.0)\n",
    "            normalized_avg_views = min(average_views_per_post / max_avg_views, 1.0)\n",
    "            normalized_avg_price = min(average_price_point / max_avg_price, 1.0)\n",
    "\n",
    "            lending_score = (normalized_posting_freq * 0.4) + \\\n",
    "                            (normalized_avg_views * 0.4) + \\\n",
    "                            (normalized_avg_price * 0.2)\n",
    "\n",
    "            vendor_scores.append({\n",
    "                'Vendor Channel': data['name'],\n",
    "                'Posts/Week': round(posting_frequency_per_week, 2),\n",
    "                'Avg. Views/Post': round(average_views_per_post, 2),\n",
    "                'Avg. Price (ETB)': round(average_price_point, 2) if average_price_point is not None else 'N/A',\n",
    "                'Top Post Product': data['top_post_details'].get('product', 'N/A'),\n",
    "                'Top Post Price (ETB)': data['top_post_details'].get('price', 'N/A'),\n",
    "                'Lending Score': round(lending_score, 4)\n",
    "            })\n",
    "\n",
    "        # --- Step 7: Present Final \"Vendor Scorecard\" Table ---\n",
    "        vendor_scorecard_df = pd.DataFrame(vendor_scores)\n",
    "        vendor_scorecard_df = vendor_scorecard_df.sort_values(by='Lending Score', ascending=False)\n",
    "\n",
    "        print(\"\\n--- FinTech Vendor Scorecard for Micro-Lending ---\")\n",
    "        print(\"\\n**Note on NER Accuracy:**\")\n",
    "        print(\"The NER model's F1-score was approximately 15% during fine-tuning due to the small labeled dataset (30-50 messages).\")\n",
    "        print(\"Therefore, the 'Top Post Product' and 'Avg. Price (ETB)' metrics derived from NER extractions may contain significant inaccuracies.\")\n",
    "        print(\"To improve the reliability of this scorecard, a substantially larger labeled dataset for NER is CRITICAL.\")\n",
    "        print(vendor_scorecard_df.to_markdown(index=False))\n",
    "\n",
    "        print(\"\\n--- Lending Score Design Notes ---\")\n",
    "        print(\"The 'Lending Score' is a simple weighted average designed for demonstration purposes. Its components are:\")\n",
    "        print(\" - Posting Frequency: Measures vendor activity.\")\n",
    "        print(\" - Average Views per Post: Indicates market reach and customer interest.\")\n",
    "        print(\" - Average Price Point: Provides insight into the vendor's product segment.\")\n",
    "        print(\"Normalization factors (e.g., max_posting_freq, max_avg_views) are heuristic and should be refined with real business data distributions.\")\n",
    "        print(\"Weights (e.g., 0.4, 0.4, 0.2) are illustrative and should be determined by FinTech experts based on lending criteria.\")\n",
    "else:\n",
    "    print(\"Skipping Vendor Scorecard calculation as NER model could not be loaded or no preprocessed messages were available.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
