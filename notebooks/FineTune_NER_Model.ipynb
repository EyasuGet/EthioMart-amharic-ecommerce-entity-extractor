{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56f7543",
   "metadata": {},
   "source": [
    "# Task3_FineTune_NER_Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd385f5",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c848d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- Step 1: Install Necessary Libraries ---\n",
    "!pip install transformers datasets seqeval accelerate evaluate\n",
    "\n",
    "# IMPORTANT: After running this cell, if prompted, click \"Restart runtime\"\n",
    "# and then \"Run all cells\" to ensure all libraries are correctly loaded.\n",
    "\n",
    "# --- Step 2: Import Libraries ---\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd # Although not strictly used in this specific notebook, good practice for ML setup\n",
    "import torch\n",
    "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from seqeval.metrics import classification_report\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734a0e4",
   "metadata": {},
   "source": [
    "# --- Configuration ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1f113",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DRIVE_PROJECT_BASE_PATH = \"/content/drive/MyDrive/colab_projects/EthioMart_NER\"\n",
    "\n",
    "# Path to your labeled CoNLL file within Google Drive\n",
    "CONLL_FILE_PATH = os.path.join(DRIVE_PROJECT_BASE_PATH, \"data/labeled_data/labeled_data.conll\")\n",
    "\n",
    "# Define your entity types (must match what you used for training)\n",
    "LABEL_NAMES = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
    "\n",
    "# --- Step 3: Load and Parse the Labeled Dataset in CoNLL Format ---\n",
    "def parse_conll_file(file_path):\n",
    "    \"\"\"Parses a CoNLL formatted file into a list of dictionaries.\"\"\"\n",
    "    try:\n",
    "        raw_text = open(file_path, \"r\", encoding=\"utf-8\").read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CoNLL file not found at {file_path}. Please upload it or check the path.\")\n",
    "        return []\n",
    "    \n",
    "    sentences = raw_text.strip().split(\"\\n\\n\")\n",
    "    data = []\n",
    "    for sentence_str in sentences:\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        lines = sentence_str.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    ner_tags.append(parts[1])\n",
    "        if tokens and ner_tags:\n",
    "            data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "    return data\n",
    "\n",
    "print(f\"Loading labeled data from {CONLL_FILE_PATH} for fine-tuning...\")\n",
    "conll_data = parse_conll_file(CONLL_FILE_PATH)\n",
    "if not conll_data:\n",
    "    print(\"No labeled data found. Fine-tuning will be skipped.\")\n",
    "    exit() # Exit if no data to train on\n",
    "\n",
    "print(f\"Loaded {len(conll_data)} sentences for fine-tuning.\")\n",
    "\n",
    "# Define features for the dataset. ClassLabel maps string labels to integers.\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=LABEL_NAMES))\n",
    "})\n",
    "\n",
    "# Create the Hugging Face Dataset object\n",
    "dataset = Dataset.from_list(conll_data, features=features)\n",
    "\n",
    "# --- Step 4: Split Data into Training and Validation Sets ---\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"\\nDataset split: {len(train_dataset)} training examples, {len(eval_dataset)} evaluation examples.\")\n",
    "print(\"Example from training dataset:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "# --- Step 5: Choose a Pre-trained Model and Tokenizer ---\n",
    "# Using the best performing model identified from previous comparison\n",
    "MODEL_NAME = \"mbeukman/xlm-roberta-base-finetuned-ner-amharic\"\n",
    "MODEL_SHORT_NAME = \"XLM-R-Amharic-NER\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer and model from: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL_NAMES),\n",
    "    id2label={i: label for i, label in enumerate(LABEL_NAMES)},\n",
    "    label2id={label: i for i, label in enumerate(LABEL_NAMES)},\n",
    "    ignore_mismatched_sizes=True # Crucial: tells the model to reinitialize the classification head\n",
    ")\n",
    "\n",
    "# Verify label mappings\n",
    "id2label = model.config.id2label # Get model's id2label after loading\n",
    "label2id = {v: k for k, v in id2label.items()} # Correct label2id mapping\n",
    "print(f\"Model's ID to Label mapping: {id2label}\")\n",
    "print(f\"Model's Label to ID mapping: {label2id}\")\n",
    "\n",
    "# --- Step 6: Tokenization and Label Alignment Function ---\n",
    "def tokenize_and_align_labels(examples, id2label_map, label2id_map):\n",
    "    \"\"\"Aligns word-level CoNLL labels to subword tokens.\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label_ids_raw in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        current_labels = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                current_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                original_label_string = id2label_map[label_ids_raw[word_idx]]\n",
    "                current_labels.append(label2id_map[original_label_string])\n",
    "            else:\n",
    "                original_label_string = id2label_map[label_ids_raw[word_idx]]\n",
    "                if original_label_string.startswith(\"B-\"):\n",
    "                    i_label_string = \"I-\" + original_label_string[2:]\n",
    "                    if i_label_string in label2id_map:\n",
    "                        current_labels.append(label2id_map[i_label_string])\n",
    "                    else:\n",
    "                        current_labels.append(label2id_map[original_label_string])\n",
    "                else:\n",
    "                    current_labels.append(label2id_map[original_label_string])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(current_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"\\nTokenizing and aligning labels...\")\n",
    "tokenized_train_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, id2label, label2id), batched=True)\n",
    "# FIX: Corrected typo from 'tokenize_and_and_align_labels' to 'tokenize_and_align_labels'\n",
    "tokenized_eval_dataset = eval_dataset.map(lambda x: tokenize_and_align_labels(x, id2label, label2id), batched=True)\n",
    "\n",
    "print(\"\\nExample of tokenized and aligned input:\")\n",
    "print(tokenized_train_dataset[0])\n",
    "\n",
    "# --- Step 7: Set Up Training Arguments ---\n",
    "# Output directory for this specific model\n",
    "OUTPUT_DIR = os.path.join(DRIVE_PROJECT_BASE_PATH, f\"{MODEL_SHORT_NAME}_ner_output\")\n",
    "LOGGING_DIR = os.path.join(DRIVE_PROJECT_BASE_PATH, f\"{MODEL_SHORT_NAME}_ner_logs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGGING_DIR, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20, # Reduced epochs for faster iteration\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    "    # Further parameter tuning considerations for optimizing results (beyond basic setup):\n",
    "    # - learning_rate: Experiment with values like 1e-5, 3e-5.\n",
    "    # - gradient_accumulation_steps: Increase if batch_size is small to simulate larger effective batch size.\n",
    "    # - lr_scheduler_type: Use \"cosine\" or \"linear\" for more sophisticated learning rate decay.\n",
    "    # - fp16: Set to True for faster training with mixed precision if GPU supports it.\n",
    "    # - num_train_epochs: Can be adjusted based on validation loss/F1-score trends (e.g., using callbacks for early stopping).\n",
    "    # - optimizer: Explore alternatives like AdamW with custom parameters.\n",
    ")\n",
    "\n",
    "# --- Step 8: Define Metrics for Evaluation ---\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Computes and returns evaluation metrics using seqeval.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[LABEL_NAMES[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[LABEL_NAMES[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# --- Step 9: Initialize Trainer ---\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- Step 10: Fine-Tune the Model ---\n",
    "print(\"\\nStarting model fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")\n",
    "\n",
    "# --- Step 11: Evaluate the Fine-tuned Model on Validation Set ---\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(eval_results)\n",
    "\n",
    "# --- Step 12: Save the Fine-tuned Model and Tokenizer ---\n",
    "FINAL_MODEL_PATH = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "print(f\"\\nSaving fine-tuned model to {FINAL_MODEL_PATH}...\")\n",
    "trainer.save_model(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "print(\"Model and tokenizer saved successfully.\")\n",
    "\n",
    "print(f\"\\nFine-tuning for {MODEL_SHORT_NAME} completed and model saved to: {FINAL_MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
