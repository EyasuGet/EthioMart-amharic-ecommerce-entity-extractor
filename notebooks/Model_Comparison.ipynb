{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81696cc",
   "metadata": {},
   "source": [
    "# Task4_Model_Comparison.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeba922",
   "metadata": {},
   "source": [
    "## import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec867e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Objective: Compare different NER models and select the best-performing one.\n",
    "\n",
    "# --- Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# --- Step 1: Install Necessary Libraries ---\n",
    "!pip install transformers datasets seqeval accelerate evaluate\n",
    "\n",
    "# IMPORTANT: After running this cell, if prompted, click \"Restart runtime\"\n",
    "# and then \"Run all cells\" to ensure all libraries are correctly loaded.\n",
    "\n",
    "# --- Step 2: Import Libraries ---\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from seqeval.metrics import classification_report\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6c270",
   "metadata": {},
   "source": [
    "## --- Configuration ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcea989",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DRIVE_PROJECT_BASE_PATH = \"/content/drive/MyDrive/colab_projects/EthioMart_NER\"\n",
    "\n",
    "# Path to your labeled CoNLL file within Google Drive\n",
    "CONLL_FILE_PATH = os.path.join(DRIVE_PROJECT_BASE_PATH, \"data/labeled_data/labeled_data.conll\")\n",
    "\n",
    "# Define your entity types (must match what you used for training)\n",
    "LABEL_NAMES = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
    "\n",
    "# --- Data Loading and Preparation (Repeated from Task 3 for independence) ---\n",
    "def parse_conll_file(file_path):\n",
    "    \"\"\"Parses a CoNLL formatted file into a list of dictionaries.\"\"\"\n",
    "    try:\n",
    "        raw_text = open(file_path, \"r\", encoding=\"utf-8\").read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CoNLL file not found at {file_path}. Please upload it or check the path.\")\n",
    "        return []\n",
    "    \n",
    "    sentences = raw_text.strip().split(\"\\n\\n\")\n",
    "    data = []\n",
    "    for sentence_str in sentences:\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        lines = sentence_str.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    ner_tags.append(parts[1])\n",
    "        if tokens and ner_tags:\n",
    "            data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "    return data\n",
    "\n",
    "print(f\"Loading labeled data from {CONLL_FILE_PATH} for fine-tuning...\")\n",
    "conll_data = parse_conll_file(CONLL_FILE_PATH)\n",
    "if not conll_data:\n",
    "    print(\"No labeled data found. Model comparison will be skipped.\")\n",
    "    exit() # Exit if no data to train on\n",
    "\n",
    "print(f\"Loaded {len(conll_data)} sentences for fine-tuning.\")\n",
    "\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=LABEL_NAMES))\n",
    "})\n",
    "dataset = Dataset.from_list(conll_data, features=features)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"\\nDataset split: {len(train_dataset)} training examples, {len(eval_dataset)} evaluation examples.\")\n",
    "\n",
    "# --- Tokenization and Label Alignment Function ---\n",
    "def tokenize_and_align_labels(examples, id2label_map, label2id_map):\n",
    "    \"\"\"Aligns word-level CoNLL labels to subword tokens.\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label_ids_raw in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        current_labels = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                current_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                original_label_string = id2label_map[label_ids_raw[word_idx]]\n",
    "                current_labels.append(label2id_map[original_label_string])\n",
    "            else:\n",
    "                original_label_string = id2label_map[label_ids_raw[word_idx]]\n",
    "                if original_label_string.startswith(\"B-\"):\n",
    "                    i_label_string = \"I-\" + original_label_string[2:]\n",
    "                    if i_label_string in label2id_map:\n",
    "                        current_labels.append(label2id_map[i_label_string])\n",
    "                    else:\n",
    "                        current_labels.append(label2id_map[original_label_string])\n",
    "                else:\n",
    "                    current_labels.append(label2id_map[original_label_string])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(current_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# --- Metrics for Evaluation ---\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Computes and returns evaluation metrics using seqeval.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[LABEL_NAMES[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[LABEL_NAMES[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# --- Main Model Training and Evaluation Function ---\n",
    "def train_and_evaluate_model(model_name: str, model_short_name: str):\n",
    "    \"\"\"Loads, fine-tunes, and evaluates a single NER model.\"\"\"\n",
    "    print(f\"\\n--- Starting fine-tuning for {model_short_name} ({model_name}) ---\")\n",
    "    \n",
    "    global tokenizer # Global tokenizer will be set per model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(LABEL_NAMES),\n",
    "        id2label={i: label for i, label in enumerate(LABEL_NAMES)},\n",
    "        label2id={label: i for i, label in enumerate(LABEL_NAMES)},\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    print(\"Tokenizing and aligning labels for current model...\")\n",
    "    tokenized_train_dataset_model = train_dataset.map(lambda x: tokenize_and_align_labels(x, model.config.id2label, model.config.label2id), batched=True)\n",
    "    tokenized_eval_dataset_model = eval_dataset.map(lambda x: tokenize_and_align_labels(x, model.config.id2label, model.config.label2id), batched=True)\n",
    "\n",
    "    # Define output and logging directories for this specific model within Google Drive\n",
    "    output_dir_model = os.path.join(DRIVE_PROJECT_BASE_PATH, f\"{model_short_name}_ner_output\")\n",
    "    logging_dir_model = os.path.join(DRIVE_PROJECT_BASE_PATH, f\"{model_short_name}_ner_logs\")\n",
    "    os.makedirs(output_dir_model, exist_ok=True)\n",
    "    os.makedirs(logging_dir_model, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir_model,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=20, # Reduced epochs for faster comparison\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=logging_dir_model,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset_model,\n",
    "        eval_dataset=tokenized_eval_dataset_model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"\\nEvaluation Results for {model_short_name}:\")\n",
    "    print(eval_results)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    final_model_path = os.path.join(output_dir_model, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    print(f\"Model and tokenizer for {model_short_name} saved to {final_model_path}\")\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_short_name,\n",
    "        \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "        \"eval_precision\": eval_results[\"eval_precision\"],\n",
    "        \"eval_recall\": eval_results[\"eval_recall\"],\n",
    "        \"eval_f1\": eval_results[\"eval_f1\"],\n",
    "        \"eval_accuracy\": eval_results[\"eval_accuracy\"],\n",
    "        \"eval_runtime\": eval_results[\"eval_runtime\"],\n",
    "    }\n",
    "\n",
    "# --- Models to Compare ---\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"XLM-R-Amharic-NER\",\n",
    "        \"id\": \"mbeukman/xlm-roberta-base-finetuned-ner-amharic\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"BERT-Medium-Amharic-NER\",\n",
    "        \"id\": \"rasyosef/bert-medium-amharic-finetuned-ner\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mBERT-Base-Cased\",\n",
    "        \"id\": \"bert-base-multilingual-cased\"\n",
    "    }\n",
    "]\n",
    "\n",
    "all_comparison_results = []\n",
    "tokenizer = None # Initialize global tokenizer for first use\n",
    "\n",
    "for config_item in model_configs:\n",
    "    results = train_and_evaluate_model(config_item[\"id\"], config_item[\"name\"])\n",
    "    all_comparison_results.append(results)\n",
    "\n",
    "# --- Compare Models and Select Best ---\n",
    "print(\"\\n--- Model Comparison Results ---\")\n",
    "results_df = pd.DataFrame(all_comparison_results)\n",
    "print(results_df.to_markdown(index=False))\n",
    "\n",
    "# Select the best model based on F1-score\n",
    "best_model_row = results_df.loc[results_df['eval_f1'].idxmax()]\n",
    "BEST_MODEL_FOR_INFERENCE_PATH = os.path.join(DRIVE_PROJECT_BASE_PATH, f\"{best_model_row['model_name']}_ner_output/final_model\")\n",
    "\n",
    "print(f\"\\nBest performing model based on F1-score: {best_model_row['model_name']}\")\n",
    "print(f\"F1-score: {best_model_row['eval_f1']:.4f}\")\n",
    "print(f\"Path to best model for subsequent tasks: {BEST_MODEL_FOR_INFERENCE_PATH}\")\n",
    "\n",
    "# Save the path to the best model for use in other notebooks (optional, for automation)\n",
    "with open(os.path.join(DRIVE_PROJECT_BASE_PATH, \"best_model_path.txt\"), \"w\") as f:\n",
    "    f.write(BEST_MODEL_FOR_INFERENCE_PATH)\n",
    "print(f\"Best model path saved to {os.path.join(DRIVE_PROJECT_BASE_PATH, 'best_model_path.txt')}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
